{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Sentiment Analyzer - Additional Experiments\n",
        "\n",
        "This notebook documents alternative preprocessing techniques and model architectures that were tested during development. These approaches were ultimately not used in the final model as they did not improve performance.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/georgehtliu/ignition-hack-2020/blob/master/submission_extras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4nXqH4Jya_XB"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5ri4yLmgap0t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import nltk\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dqWFhsaCd7LG"
      },
      "source": [
        "## Alternative Preprocessing Techniques\n",
        "\n",
        "The following preprocessing techniques were tested but ultimately resulted in lower F1 scores compared to simple punctuation removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "80Y34j4mccIs"
      },
      "source": [
        "### Lemmatization with Part-of-Speech Tagging\n",
        "\n",
        "**Result:** Significantly increases training time and decreases F1 scores by ~1%.\n",
        "\n",
        "Not recommended for this use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "j96CReUccw3d"
      },
      "outputs": [],
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Get the WordNet part-of-speech tag for a word.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dictionary = {\n",
        "        \"J\": wordnet.ADJ,\n",
        "        \"N\": wordnet.NOUN,\n",
        "        \"V\": wordnet.VERB,\n",
        "        \"R\": wordnet.ADV\n",
        "    }\n",
        "    return tag_dictionary.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def wn_lemmatize(sentence, lemmatizer):\n",
        "    \"\"\"Lemmatize words in a sentence using POS tagging.\"\"\"\n",
        "    words_list = sentence.split()\n",
        "    for i in range(len(words_list)):\n",
        "        if len(words_list[i]) >= 1:\n",
        "            words_list[i] = lemmatizer.lemmatize(\n",
        "                words_list[i], \n",
        "                get_wordnet_pos(words_list[i])\n",
        "            )\n",
        "    return ' '.join(words_list)\n",
        "\n",
        "# Usage example (not used in final model):\n",
        "# df[\"Text\"] = df['Text'].apply(lambda sentence: wn_lemmatize(sentence, lemmatizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "53wHaJLjcAdn"
      },
      "source": [
        "### Name Lemmatization / Generalization\n",
        "\n",
        "**Result:** Slightly decreases F1 scores.\n",
        "\n",
        "Removes @mentions and #hashtags from text, but this removal reduces model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MaSsn8QucJcX"
      },
      "outputs": [],
      "source": [
        "def lemmatize_name(text):\n",
        "    \"\"\"\n",
        "    Remove @mentions and #hashtags from the beginning of text.\n",
        "    \n",
        "    Note: This approach decreased F1 scores and was not used in the final model.\n",
        "    \"\"\"\n",
        "    if len(text) > 0 and (text[0] == '@' or text[0] == '#'):\n",
        "        words = text.split()\n",
        "        if len(words) > 0:\n",
        "            words[0] = ''\n",
        "        return ' '.join(words)\n",
        "    return text\n",
        "\n",
        "# Usage example (not used in final model):\n",
        "# df['Text'] = df['Text'].map(lambda text: lemmatize_name(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RBBd0__ZbQE7"
      },
      "source": [
        "### Stop Word Removal\n",
        "\n",
        "**Result:** Decreases F1 scores by ~1%.\n",
        "\n",
        "Removing common stop words reduced model performance, likely because they provide context for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lp7jTAf2bVHC"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove English stop words from text.\"\"\"\n",
        "    words = text.split() if isinstance(text, str) else []\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Usage example (not used in final model):\n",
        "# df['Text'] = df['Text'].apply(lambda x: remove_stopwords(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NvjbbW38bb_m"
      },
      "source": [
        "### Punctuation Removal\n",
        "\n",
        "**Note:** While the vectorizer has built-in functionality for handling punctuation, we found that explicit preprocessing improved performance in our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "38BzhpambsGa"
      },
      "outputs": [],
      "source": [
        "def remove_punct(text):\n",
        "    \"\"\"Remove punctuation and numbers from text.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = \"\".join([char for char in str(text) if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "\n",
        "# This preprocessing step IS used in the final model:\n",
        "# df['Text'] = df['Text'].map(lambda text: remove_punct(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "phIamz93b0uU"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "**Note:** Redundant due to vectorizer's built-in tokenization functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UFR_i5L0b7Br"
      },
      "outputs": [],
      "source": [
        "def tokenization(text):\n",
        "    \"\"\"Tokenize text using regex (not needed - vectorizer handles this).\"\"\"\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens\n",
        "\n",
        "# Usage example (not needed - TF-IDF vectorizer handles tokenization):\n",
        "# df['Text'] = df['Text'].map(lambda text: tokenization(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3HjneTvxeJbC"
      },
      "source": [
        "## Alternative Classifiers\n",
        "\n",
        "The following classifiers were tested but did not outperform Logistic Regression for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jYURI4s8eN83"
      },
      "source": [
        "### Neural Network (MLPClassifier)\n",
        "\n",
        "**Result:** Very slow to train, mediocre accuracy compared to Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xgxNGd-neVSn"
      },
      "outputs": [],
      "source": [
        "# Multi-layer Perceptron (Neural Network)\n",
        "# Note: Requires X_train_vectors, y_train, X_test_vectors, y_test to be defined\n",
        "# (from train_test_split in the main training notebook)\n",
        "\n",
        "clf_nn = MLPClassifier(\n",
        "    solver='adam', \n",
        "    activation='relu', \n",
        "    hidden_layer_sizes=(64, 64),\n",
        "    random_state=42\n",
        ")\n",
        "clf_nn.fit(X_train_vectors, y_train)\n",
        "predictions = clf_nn.predict(X_test_vectors)\n",
        "print(f\"F1 Score: {f1_score(y_test, predictions, average='weighted')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2HslGNGceaXa"
      },
      "source": [
        "### Decision Tree Classifier\n",
        "\n",
        "**Result:** Sub-par accuracy compared to Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_NSQSWkSeutt"
      },
      "outputs": [],
      "source": [
        "# Decision Tree with GridSearchCV\n",
        "parameters_dt = {\n",
        "    'criterion': ('gini', 'entropy'),\n",
        "    'splitter': ('best', 'random'),\n",
        "    'max_depth': (None, 4, 100, 1000)\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "clf_dt = GridSearchCV(dt, parameters_dt, cv=5, scoring='f1_weighted')\n",
        "clf_dt.fit(X_train_vectors, y_train)\n",
        "\n",
        "predictions = clf_dt.predict(X_test_vectors)\n",
        "print(f\"Best Parameters: {clf_dt.best_params_}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, predictions, average='weighted')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cVQmv-8newLy"
      },
      "source": [
        "### Support Vector Machine (SVM)\n",
        "\n",
        "**Result:** Incapable of handling large datasets efficiently. Good accuracy for smaller datasets, but not scalable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4013Z6bge_-v"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM)\n",
        "# Note: This requires a smaller subset of data due to memory constraints\n",
        "# Around 68% accuracy using 8000 of the 1M training examples\n",
        "\n",
        "clf_svm = SVC(\n",
        "    kernel='rbf', \n",
        "    C=4, \n",
        "    decision_function_shape='ovo',\n",
        "    random_state=42\n",
        ")\n",
        "clf_svm.fit(X_train_vectors, y_train)\n",
        "\n",
        "predictions = clf_svm.predict(X_test_vectors)\n",
        "print(f\"F1 Score: {f1_score(y_test, predictions, average='weighted')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1SC27BmafG6H"
      },
      "source": [
        "### Stochastic Gradient Descent (SGD) Classifier\n",
        "\n",
        "**Result:** Very fast to train, but does not improve much as dataset size increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TOs34XHCfPRi"
      },
      "outputs": [],
      "source": [
        "# SGD Classifier with logistic loss\n",
        "clf_sgd = SGDClassifier(\n",
        "    loss='log',\n",
        "    penalty='elasticnet',\n",
        "    l1_ratio=0.05,\n",
        "    random_state=42\n",
        ")\n",
        "clf_sgd.fit(X_train_vectors, y_train)\n",
        "\n",
        "predictions = clf_sgd.predict(X_test_vectors)\n",
        "print(f\"F1 Score: {f1_score(y_test, predictions, average='weighted')}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOgvH6e92uxcQW6stPO8/G9",
      "collapsed_sections": [
        "4nXqH4Jya_XB",
        "80Y34j4mccIs",
        "53wHaJLjcAdn",
        "RBBd0__ZbQE7",
        "NvjbbW38bb_m",
        "phIamz93b0uU",
        "jYURI4s8eN83",
        "2HslGNGceaXa",
        "cVQmv-8newLy",
        "1SC27BmafG6H"
      ],
      "include_colab_link": true,
      "name": "submission_extras.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
