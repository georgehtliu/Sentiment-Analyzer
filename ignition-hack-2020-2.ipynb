{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationsEvaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgehtliu/ignition-hack-2020/blob/master/ignition-hack-2020-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wsOBPi-D-fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import nltk \n",
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMto452BN0Kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0052ec8d-4caf-43f5-81f8-10b4d97ee455"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = input(\"Please enter the path to your training_data.csv file in your Google Drive. (Right click the file on the left and click copy path and paste it in here.)\")\n",
        "df = pd.read_csv(path)\n",
        "df = df[['Text','Sentiment']]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Please enter the path to your training_data.csv file in your Google Drive. (Right click the file on the left and click copy path and paste it in here.)/content/drive/My Drive/Colab Notebooks/Contestant Accessible/Division Sigma/training_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqsr84JXjKvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Lemitize, Remove Punctuation, Tokenize \n",
        "def lemitize(text):\n",
        "    if text[0] == '@' or text[0]=='#':\n",
        "        L = text.split()\n",
        "        L[0] = ''\n",
        "        return ' '.join(L)\n",
        "    return text\n",
        "\n",
        "def remove_punct(text):\n",
        "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "\n",
        "def tokenization(text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text\n",
        "\n",
        "df['Text'] = df['Text'].map(lambda text: lemitize(text))\n",
        "df['Text'] = df['Text'].map(lambda text: remove_punct(text))\n",
        "df['Text'] = df['Text'].map(lambda text: tokenization(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8lJLy4rnSf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0f2e4a74-89ae-40f9-9d30-f8f26dcac75e"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Qk3qeqkcYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "209ad438-4cb5-4c26-eafe-da9aa683c9b1"
      },
      "source": [
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    text = [word for word in text if word not in stopword]\n",
        "    return ' '.join(text)\n",
        "\n",
        "df['Text'] = df['Text'].apply(lambda x: remove_stopwords(x))\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I heart filling dennisschaub desk means sales ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>people create prettier younger better looking...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>way dont want tour end</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi Amazing Brother Sending Limitless Love You...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>chocolate</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Just got home And Ive heard MY LAKERS BEAT TH...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>yup yup especially</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>new love matt lanter absolutely adore ah swoon...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Grahams car looked really quotsnakeyquot replays</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Why people make evil towards I hate</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Sentiment\n",
              "0  I heart filling dennisschaub desk means sales ...          1\n",
              "1   people create prettier younger better looking...          1\n",
              "2                            way dont want tour end           0\n",
              "3   Hi Amazing Brother Sending Limitless Love You...          1\n",
              "4                                          chocolate          1\n",
              "5   Just got home And Ive heard MY LAKERS BEAT TH...          1\n",
              "6                                 yup yup especially          0\n",
              "7  new love matt lanter absolutely adore ah swoon...          1\n",
              "8  Grahams car looked really quotsnakeyquot replays           0\n",
              "9               Why people make evil towards I hate           0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NrmrDezbOkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword_list = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMmiTRAWFxbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Data is split 50-50 between 0-1\n",
        "# df = pd.read_csv('training_data.csv')\n",
        "# df = df[['Text','Sentiment']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omVgHxpSF2_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mini_df = df\n",
        "\n",
        "X = np.array(mini_df['Text'])\n",
        "y = np.array(mini_df['Sentiment'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftjoFHeFG8Px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tfidf works better than count vectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=None)\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "X_test_vectors = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0f8nKNvXgXR",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHkxy4BnG-Jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "213ff80f-0ad7-492a-cd49-4a4b73f3b474"
      },
      "source": [
        "clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(64,64))\n",
        "clf.fit(X_train_vectors, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(64, 64), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu_pyU46G_tM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6c82f0c-b319-456e-dc3d-1cd745a13117"
      },
      "source": [
        "print(f1_score(y_test, clf.predict(X_test_vectors), average=None, labels=[0,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.69952464 0.69997502]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC6UdFJwTol0",
        "colab_type": "text"
      },
      "source": [
        "#Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeIvWpm7ToHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "parameters_dt = {'criterion': ('gini', 'entropy'), 'splitter': ('best', 'random'), 'max_depth': (None, 4,100,1000)}\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "clf_dt = GridSearchCV(dt, parameters_dt, cv = 5)\n",
        "\n",
        "clf_dt.fit(X_train_vectors, y_train)\n",
        "\n",
        "print(f1_score(y_test, clf_dt.predict(X_test_vectors), average=None, labels=[0,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6YzEWPLdrhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXmKpZa6dkfH",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21QFHC5Udy_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZsTRUdhUJaj",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Pu_OAOsoY8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d92RTz8jUQCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "clf_gnb = GaussianNB()\n",
        "clf_gnb.fit(X_train_vectors.toarray(), y_train)\n",
        "\n",
        "print(f1_score(y_test, clf_gnb.predict(X_test_vectors.toarray())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL3c3B_Acnco",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ietN61NAcqqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2f563f6-a7a9-4a3c-f916-f1c54ae98754"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\"\"\"\n",
        "parameters_log = {'C': (1.0,4.0,8.0,16.0,32.0), 'solver': ('sag', 'saga', 'lbfgs', 'newton-cg')}\n",
        "log = LogisticRegression(max_iter=1000)\n",
        "\n",
        "clf_log = GridSearchCV(log, parameters_log, cv=5)\n",
        "\n",
        "clf_log.fit(X_train_vectors, y_train)\n",
        "\"\"\"\n",
        "clf_log = LogisticRegression(solver='sag')\n",
        "clf_log.fit(X_train_vectors, y_train)\n",
        "\n",
        "print(f1_score(y_test, clf_log.predict(X_test_vectors), average=None, labels=[0,1]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.79548088 0.80023326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thQwYfKWrzbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "511d7676-ab29-4c57-81fc-f8138f4b199f"
      },
      "source": [
        "print(clf_log.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3a0f588527f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'best_params_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJoCMyN2jsKi",
        "colab_type": "text"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q3MrhJ7jSk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af7b295b-8021-42a0-ee24-07ed1e2f517b"
      },
      "source": [
        "clf = SVC(kernel='rbf', C=4, decision_function_shape='ovo')\n",
        "clf.fit(X_train_vectors, y_train)\n",
        "\n",
        "\n",
        "\n",
        "## Around 68% accuracy using 8000 of the 1M training examples\n",
        "print(f1_score(y_test, clf.predict(X_test_vectors), average=None, labels=[0,1]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.73204279 0.74177712]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaBVQl7awn2a",
        "colab_type": "text"
      },
      "source": [
        "SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKIisBGIwsaq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "124fc5b5-ac5d-4006-8f50-ded676b40b8d"
      },
      "source": [
        "sgd = SGDClassifier(loss='log',penalty='elasticnet',l1_ratio=0.05)\n",
        "sgd.fit(X_train_vectors, y_train)\n",
        "\n",
        "print(f1_score(y_test, sgd.predict(X_test_vectors), average=None, labels=[0,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.75113432 0.76210827]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}